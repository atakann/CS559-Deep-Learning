{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A different approach.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Hq0HC4TNqynW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "67890876-115a-4f5a-db1d-df97f69dd5c3"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "print(tf.__version__)\n",
        "\n",
        "folder = \"/content/gdrive/My Drive/UTKFace_downsampled/nparrays\"\n",
        "    \n",
        "training_images = np.load(os.path.join(folder, 'training.npy'))\n",
        "training_labels = np.load(os.path.join(folder, 'training_labels.npy'))\n",
        "training_labels = training_labels.reshape([5400 , 1])\n",
        "\n",
        "validation_images = np.load(os.path.join(folder, 'validation.npy'))\n",
        "validation_labels = np.load(os.path.join(folder, 'validation_labels.npy'))\n",
        "validation_labels = validation_labels.reshape([2315 , 1])\n",
        "\n",
        "test_images = np.load(os.path.join(folder, 'test.npy'))\n",
        "test_labels = np.load(os.path.join(folder, 'test_labels.npy'))\n",
        "test_labels = test_labels.reshape([1159 , 1])\n",
        "\n",
        "training_labels = np.array(training_labels, dtype=np.float32)\n",
        "validation_labels = np.array(validation_labels, dtype=np.float32)\n",
        "test_labels = np.array(test_labels, dtype=np.float32)\n",
        "\n",
        "\n",
        "#for img in training_images:\n",
        "#  img = tf.to_float(img)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "1.13.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "y55Yn7Rjq2bo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "s = np.arange(0, 5400, 1)\n",
        "np.random.shuffle(s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pZlpZOWIq2y7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cnn_model_fn():\n",
        "  \"\"\"Model function for CNN.\"\"\"\n",
        "  # Input Layer\n",
        "  x = tf.placeholder(tf.float32, shape = (None, 91, 91, 1))\n",
        "  input_layer = tf.reshape(x, [-1, 91, 91, 1])\n",
        "  y = tf.placeholder(tf.float32, shape = (None , 1))\n",
        "  #z = tf.placeholder(tf.bool)\n",
        "\n",
        "  # Convolutional Layer #1\n",
        "  conv1 = tf.contrib.layers.conv2d(\n",
        "      inputs=input_layer,\n",
        "      num_outputs=32,\n",
        "      kernel_size=[3, 3],\n",
        "      weights_initializer=tf.initializers.truncated_normal\n",
        "      )\n",
        "  conv2 = tf.contrib.layers.conv2d(\n",
        "      inputs=input_layer,\n",
        "      num_outputs=32,\n",
        "      kernel_size=[3, 3],\n",
        "      weights_initializer=tf.initializers.truncated_normal\n",
        "      )\n",
        "  \n",
        "  bn000 = tf.contrib.layers.batch_norm(\n",
        "    inputs = conv2,\n",
        "  )\n",
        "  # Pooling Layer #1\n",
        "  pool1 = tf.contrib.layers.max_pool2d(inputs=bn000, kernel_size=[2, 2], stride=2)\n",
        "\n",
        "  # Convolutional Layer #2 and Pooling Layer #2\n",
        "  conv3 = tf.contrib.layers.conv2d(\n",
        "      inputs=pool1,\n",
        "      num_outputs=64,\n",
        "      kernel_size=[3, 3],\n",
        "      weights_initializer=tf.initializers.truncated_normal\n",
        "      )\n",
        "  conv4 = tf.contrib.layers.conv2d(\n",
        "      inputs=conv3,\n",
        "      num_outputs=64,\n",
        "      kernel_size=[3, 3],\n",
        "      weights_initializer=tf.initializers.truncated_normal\n",
        "      )\n",
        "  \n",
        "  bn000 = tf.contrib.layers.batch_norm(\n",
        "    inputs = conv4,\n",
        "  )\n",
        "  \n",
        "  pool2 = tf.contrib.layers.max_pool2d(inputs=bn000, kernel_size=[2, 2], stride=2)\n",
        "  \n",
        "  conv5 = tf.contrib.layers.conv2d(\n",
        "      inputs=pool2,\n",
        "      num_outputs=128,\n",
        "      kernel_size=[3, 3],\n",
        "      weights_initializer=tf.initializers.truncated_normal\n",
        "      )\n",
        "  conv6 = tf.contrib.layers.conv2d(\n",
        "      inputs=conv5,\n",
        "      num_outputs=128,\n",
        "      kernel_size=[3, 3],\n",
        "      weights_initializer=tf.initializers.truncated_normal\n",
        "      )\n",
        "  \n",
        "  bn00 = tf.contrib.layers.batch_norm(\n",
        "    inputs = conv6\n",
        "  )\n",
        "  \n",
        "  #bn00 = tf.contrib.layers.batch_norm(\n",
        "  #  inputs = conv2,\n",
        "  #)\n",
        "  #conv3 = tf.contrib.layers.conv2d(\n",
        "  #    inputs=conv2,\n",
        "  #    num_outputs=64,\n",
        "  #    kernel_size=[5, 5],\n",
        "  #    )\n",
        "  #bn0 = tf.contrib.layers.batch_norm(\n",
        "  #  inputs = conv3,\n",
        "  #)\n",
        "  \n",
        "  #conv4 = tf.contrib.layers.conv2d(\n",
        "  #    inputs=conv3,\n",
        "  #    num_outputs=64,\n",
        "  #    kernel_size=[5, 5],\n",
        "  #    )\n",
        "  \n",
        "  #dropout0 = tf.layers.dropout(inputs=conv4, rate=0.2)\n",
        "  #Pooling layer #2\n",
        "  #pool2 = tf.contrib.layers.max_pool2d(inputs=conv2, kernel_size=[2, 2], stride=2)\n",
        "\n",
        "  # Dense Layer\n",
        "  pool2_flat = tf.contrib.layers.flatten(\n",
        "    inputs = bn00,\n",
        "    outputs_collections=None,\n",
        "    scope=None\n",
        "  )\n",
        "\n",
        "  #pool2_flat = tf.reshape(pool2, [-1, 23 * 23 * 64])\n",
        "  fc = tf.contrib.layers.fully_connected(inputs=pool2_flat, num_outputs=256, activation_fn=tf.nn.relu, weights_initializer=tf.initializers.truncated_normal) #256 normalde\n",
        "  \n",
        "  bn = tf.contrib.layers.batch_norm(\n",
        "    inputs = fc,\n",
        "  )\n",
        "  dropout = tf.layers.dropout(inputs=bn, rate=0.25)\n",
        "  fc2 = tf.contrib.layers.fully_connected(inputs=dropout, num_outputs=128, activation_fn=tf.nn.relu, weights_initializer=tf.initializers.truncated_normal)\n",
        "  \n",
        "  bn2 = tf.contrib.layers.batch_norm(\n",
        "    inputs = fc2,\n",
        "  )\n",
        "  dropout2 = tf.layers.dropout(inputs=bn2, rate=0.25)\n",
        "  \n",
        "  fc3 = tf.contrib.layers.fully_connected(inputs=dropout2, num_outputs=128, activation_fn=tf.nn.relu, weights_initializer=tf.initializers.truncated_normal)\n",
        "  \n",
        "  bn3 = tf.contrib.layers.batch_norm(\n",
        "    inputs = fc3,\n",
        "  )\n",
        "  dropout3 = tf.layers.dropout(inputs=bn3, rate=0.25)\n",
        "  \n",
        "  fc4 = tf.contrib.layers.fully_connected(inputs=dropout3, num_outputs=100, activation_fn=tf.nn.relu, weights_initializer=tf.initializers.truncated_normal,)\n",
        "  \n",
        "  dropout3 = tf.layers.dropout(inputs=fc4, rate=0.25) \n",
        "  bn4 = tf.contrib.layers.batch_norm(\n",
        "    inputs = dropout3,\n",
        "  )\n",
        "  \n",
        "  fc5 = tf.contrib.layers.fully_connected(inputs=bn4, num_outputs=1)\n",
        "  \n",
        "\n",
        "  #optimizer = tf.train.AdamOptimizer()\n",
        "  loss = tf.losses.mean_squared_error(\n",
        "    labels = y,\n",
        "    predictions = fc5,\n",
        "  )\n",
        "  loss2 = tf.losses.absolute_difference(\n",
        "    labels = y,\n",
        "    predictions = fc5,\n",
        "  )\n",
        "  \n",
        "  #regularization_losses = tf.losses.get_regularization_losses()\n",
        "  #reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
        "  \n",
        "  #loss = loss + regularization_losses\n",
        "  #loss2 = loss2 + regularization_losses\n",
        "  \n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate = 0.01)\n",
        "\n",
        "  #update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "  trainer = optimizer.minimize(loss)\n",
        "  #trainer = tf.group([trainer, update_ops])\n",
        "  \n",
        "  init = tf.global_variables_initializer()\n",
        "  \n",
        "  validation_losses = []\n",
        "  train_losses = []\n",
        "  test_losses = []\n",
        "  \n",
        "  with tf.Session() as session:                    # Create a session and print the output\n",
        "    session.run(init)                           \n",
        "    \n",
        "    for epoch in range(101):\n",
        "      \n",
        "      np.random.shuffle(s)\n",
        "      training_images_shuffled = training_images[s]\n",
        "      training_labels_shuffled = training_labels[s]\n",
        "      train_batch_losses = []\n",
        "      for i in range(0,5400,108):\n",
        "        \n",
        "        _, train_loss, loss_function = session.run([trainer, loss2, loss], feed_dict = {x: training_images_shuffled[i:i+108], y: training_labels_shuffled[i:i+108]})\n",
        "        train_batch_losses.append(train_loss)\n",
        "        \n",
        "      train_losses.append(np.mean(np.array(train_batch_losses)))\n",
        "      trainprint = np.mean(np.array(train_batch_losses))\n",
        "      if epoch % 1 == 0:\n",
        "        print(\"Loss after epoch %i: train loss: %f , loss function: %f \" %(epoch, trainprint, loss_function))\n",
        "      \n",
        "      if epoch % 1 == 0:\n",
        "        sum = 0\n",
        "        \n",
        "        #validation_batch_losses = []\n",
        "        test_batch_losses = []\n",
        "        for i in range(0, 1159, 61): \n",
        "          loss_function = session.run([loss2], feed_dict = {x: test_images[i:i+61], y: test_labels[i:i+61]})\n",
        "          #validation_batch_losses.append(loss_function)\n",
        "          test_batch_losses.append(loss_function)\n",
        "        \n",
        "        test_losses.append(np.mean(np.array(test_batch_losses)))\n",
        "        testprint = np.mean(np.array(test_batch_losses))\n",
        "        print(\"test loss:\", testprint )\n",
        "        \n",
        "        #validation_losses.append(np.mean(np.array(validation_batch_losses)))\n",
        "        #validationprint = np.mean(np.array(validation_batch_losses))\n",
        "        #print(\"validation loss:\", validationprint )\n",
        "        \n",
        "    plt.plot(np.squeeze(train_losses), color = 'b')\n",
        "    plt.plot(np.squeeze(test_losses), color = 'r')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('iterations')\n",
        "    plt.title('Learning rate= ' + str(0.001))\n",
        "    plt.show()\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zsZOsF2ksjux",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "1183e17c-8942-41b5-e0f1-df99f57f0523"
      },
      "cell_type": "code",
      "source": [
        "cnn_model_fn()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss after epoch 0: train loss: 19.739241 , loss function: 266.730957 \n",
            "test loss: 12.353051\n",
            "Loss after epoch 1: train loss: 9.438369 , loss function: 136.459137 \n",
            "test loss: 8.397021\n",
            "Loss after epoch 2: train loss: 7.368665 , loss function: 85.322266 \n",
            "test loss: 7.926374\n",
            "Loss after epoch 3: train loss: 6.244057 , loss function: 71.725494 \n",
            "test loss: 7.798539\n",
            "Loss after epoch 4: train loss: 5.067770 , loss function: 45.333187 \n",
            "test loss: 7.671678\n",
            "Loss after epoch 5: train loss: 4.438570 , loss function: 39.110966 \n",
            "test loss: 8.124886\n",
            "Loss after epoch 6: train loss: 4.702428 , loss function: 53.218502 \n",
            "test loss: 8.06267\n",
            "Loss after epoch 7: train loss: 4.717313 , loss function: 26.684727 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2diLCsiZ70uD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}